---
title: "Webscrapping"
output:
  html_document:
    df_print: paged
  pdf_document: 
    latex_engine: xelatex
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}

library(rvest)
library(dplyr)
library(httr2)
library(ggplot2)

```

El **web scraping** es una técnica utilizada para extraer información de páginas web. Para acceder a una página web desde R se suele utilizar el paquete `rvest` el cual nos permite hacer una solicitud (request) al servidor deseado mediante la función `read_html()`

## Scrappeando tablas

Un uso actual del webscrapping muy común es lo que tiene que ver conlos *football analytics*. Supongamos que nos interesa explorar la página <https://fbref.com/> para ver cómo quedó la tabla de la Primera División Argentina.

Primero, creamos un objeto con toda la info de la tabla. Es decir, extraemos el html

```{r}

url <- "https://fbref.com/es/comps/21/Estadisticas-de-Liga-Profesional-Argentina"

pagina_fbref <- read_html(url)

pagina_fbref

```

`rvest` tiene la función `html_table` que nos permite extraer tablas de cualquier página web. Veamos que nos devuelve

```{r}
tablas_1_div <- html_table(pagina_fbref)
tablas_1_div
```
```{r}
tabla_liga <- tablas_1_div[[1]]
tabla_liga
```
```{r}
tabla_localia = tablas_1_div[[2]]
tabla_localia
```
```{r}
colnames(tabla_localia) <- paste(colnames(tabla_localia), tabla_localia[1,])
tabla_localia <- tabla_localia[-1,]
tabla_localia
```


## Analizando requests

**httr2** es una librería que permite enviar solicitudes a través del protocolo HTTPS.

El comando **request()** es el que vamos a usar para traer la información de una página. Cada vez que ejecutamos el comando get estamos enviando una consulta al servidor. Hay que ser responsables en el uso de las consultas ya que si se ejecutan demasiadas pueden generar problemas en el servidor o incluso pueden bloquear nuestra IP por uso indebido.

En este caso, vamos a tratar de sacar los titulares de Pagina12.


```{r}

p12 = request("https://www.pagina12.com.ar/")

req_perform(p12)

```
```{r}
pagina12 <- read_html("https://www.pagina12.com.ar/")
pagina12
```
```{r}
elementos_h2 <- html_elements(pagina12,"h2")
elementos_h2
```

```{r}
elementos_h2[1]
```
```{r}
html_text(elementos_h2)
```

```{r}
titulares_p12 <- read_html("https://www.pagina12.com.ar/") %>%
  html_elements("h2") %>%
  html_text()

titulares_p12
```


## API Transporte BA - ECO BICIS

El gobierno de la Ciudad de Buenos Aires tiene una API mas o menos piola para levantar datos sobre colectivos, subtes, trenes y bicis en el ámbito de la Ciudad <https://api-transporte.buenosaires.gob.ar/>


Para poder acceder a este tipo de APIs, es común tener que registrarse para obtener algún tipo de credencial. Se pueden registrar acá: <https://api-transporte.buenosaires.gob.ar/registro>

Ahora si, vamos a probar descargar la información sobre las estaciones de eco bici de Buenos Aires

```{r}

link_api='https://apitransporte.buenosaires.gob.ar/ecobici/gbfs/stationInformation?'
# response = request(link_api)
# response %>% 
#   req_perform()


```

Estamos desautorizados (?) porque no pusimos las credenciales. Para hacerlo, tenemos que incluir las mismas dentro del parámetro **params**

```{r}
response <- request(link_api) |> 
  req_url_query(client_id = "f3429f52737b4e019607007f7454602f",
               client_secret= "E09120C8BC17468fBd98Bd7F37173674")%>% 
  req_perform()
```

```{r}


credenciales <- list(
  client_id = "f3429f52737b4e019607007f7454602f",
  client_secret = "E09120C8BC17468fBd98Bd7F37173674"
)

response <- request(link_api) %>% 
  req_url_query(!!!credenciales) %>% 
  req_perform()


```

La vieja usanza:

```{r}
library(httr)

credenciales <- list(
  client_id = "f3429f52737b4e019607007f7454602f",
  client_secret = "E09120C8BC17468fBd98Bd7F37173674"
)

response <- GET(link_api, query = credenciales)

status_code(response)

```

Para poder abrirlo, vamos a tener que usar **json()** y veremos la estructura de los datos.

```{r}

library(jsonlite)

response <- request(link_api) |> 
  req_url_query(client_id = "f3429f52737b4e019607007f7454602f",
               client_secret= "E09120C8BC17468fBd98Bd7F37173674")%>% 
  req_perform()

estaciones_info = response %>%
  resp_body_json(simplifyVector = T)
 
estaciones_info_df <-as.data.frame(estaciones_info$data$stations)

estaciones_info$data$stations
```

Como ya es costumbre, lxs invito a chusmear los datos y limpiarlos un poco. Nos va a interesar quedarnos con el ID, la dirección, la capacidad, el código postal, el barrio, latitud y longitud de la estacion


## Vamos a trabajar con algunas preguntas concretas:

1.  ¿Cuál es el barrio con más estaciones?
2.  ¿Cuál es el barrio con más capacidad en promedio?
3.  ¿Hay una relación entre la cantidad de estaciones y la capacidad?

1 ¿Cuál es el barrio con más estaciones? La variable "groups" es una lista, asi que vamos a tener que desarmarla con as.character()

```{r}
estaciones_info_df = estaciones_info_df %>%
  select(station_id, name, address, capacity, groups, lat, lon)

estaciones_info_df
```
```{r}
estaciones_info_df <- estaciones_info_df %>%
  mutate(groups = as.character(groups))

estaciones_info_df %>%
  group_by(groups) %>%
  summarise(cant_estaciones = n()) %>%
  arrange(desc(cant_estaciones))
```
El barrio con más estaciones es Palermo.

2.  ¿Cuál es el barrio con más capacidad en promedio?

```{r}


```

## Vamos a analizar la información actual (tiempo real) de las estaciones

Algo lindo sería analizar si hay estaciones o barrios que les vendría bien tener más bicis. Para eso, vamos a tener que ir al endpoint <https://apitransporte.buenosaires.gob.ar/ecobici/gbfs/stationStatus>?

Vamos a responder las siguientes preguntas:

1.  ¿Cómo calcularías la disponibilidad de bicis de una estación? Es decir, ¿Cuán usada es una estación?

```{r}

```


2.  ¿Las estaciones con más capacidad son las más usadas?

3.  ¿Cómo podríamos identificar barrios con estaciones que sean poco usadas?

Una cosa importante para comentar es que las apis son básicamente páginas web que reciben parámetros. No es necesario tener programas especiales ni nada raro para conectarse

Por ejemplo, para acceder a los datos de la información de estaciones de eco bici, podríamos entrar en esta web: <https://apitransporte.buenosaires.gob.ar/ecobici/gbfs/stationInformation?client_id=f3429f52737b4e019607007f7454602f&client_secret=E09120C8BC17468fBd98Bd7F37173674> que tiene el cliente y el secret metido en el medio.

Sin embargo, acceder mediante código es clave cuando queremos volver más reproducible o automatizable el flujo de trabajo. Por ejemplo, este repo actualiza cada una hora la info de las estaciones de ecobici de ciudad universitaria <https://github.com/FedeGiovannetti/Eco_bici_Ciudad_Universitaria>

Si alguien quiere sumarse a hacer algo con esta idea super simple (por ej, poner a disposición algun dashboard con info de las estaciones de Ciudad Universitaria) me avisan [giovannettipsi\@gmail.com](mailto:giovannettipsi@gmail.com){.email}

### ¿Qué días se usa más la bici?

```{r}

eco_bici_ciudad = read.csv("https://raw.githubusercontent.com/FedeGiovannetti/Eco_bici_Ciudad_Universitaria/refs/heads/main/data/full_data.csv")


eco_bici_ciudad %>% 
  mutate(dia = factor(dia, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  group_by(dia) %>% 
  summarise(media_bicis_disponibles = mean(num_bikes_available)) %>% 
  
  ggplot(aes(x = dia, y = media_bicis_disponibles)) +
  geom_col()






```

### ¿Cómo cambia la disponibilidad a lo largo del día?

```{r}
library(hms)
library(lubridate)

eco_bici_ciudad %>% 

  mutate(hora = hour(as_hms(hora))) %>% 
  group_by(hora) %>% 
  summarise(media_bicis_disponibles = mean(num_bikes_available)) %>% 
  
  ggplot(aes(x = hora, y = media_bicis_disponibles)) +
  geom_col()+
  theme_minimal()+
  scale_x_continuous(breaks = c(0:23))


```
